{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **MYMENSINGH**"],"metadata":{"id":"QkqOKFYp10CQ"}},{"cell_type":"code","source":["# ===============================\n","#  Bangla Dialect ‚Üí Standard Bangla using mBART-50 (Improved)\n","# ===============================\n","\n","!pip install transformers[sentencepiece] datasets sacrebleu evaluate torch pandas openpyxl --quiet\n","\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","import torch\n","import gc # <-- Added for memory cleanup\n","from google.colab import drive\n","\n","print(\"‚úÖ Libraries installed and imported successfully.\")\n","#drive.mount('/content/drive')\n","\n","# ===============================\n","#  Load Dataset\n","# ===============================\n","\n","try:\n","    df = pd.read_excel(\"/content/bangla_dialect_aligned_18920.xlsx\")\n","    print(\"‚úÖ Successfully loaded the dataset\")\n","except FileNotFoundError:\n","    print(\"‚ö†Ô∏è Dataset not found ‚Äî using sample data.\")\n","    df = pd.DataFrame({\n","        'Standard_Bangla': [\"‡¶∏‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Barisal': [\"‡¶π‡ßá‡¶á ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Chittagong': [\"‡¶π‡ßá‡¶á ‡¶∏‡ßç‡¶ï‡ßã‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Sylhet': [\"‡¶§‡¶æ‡¶∞‡ßá ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"]\n","    })\n","\n","# Choose dialects to train\n","DIALECTS_TO_TRAIN = ['Barisal'] # Can now train multiple, one after another\n","\n","# ===============================\n","#  Load Tokenizer (mBART)\n","# ===============================\n","\n","MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n","\n","# Load tokenizer once, it can be reused\n","# For mBART, we set the src/tgt lang on the tokenizer instance\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.src_lang = \"bn_IN\"\n","tokenizer.tgt_lang = \"bn_IN\"\n","\n","print(f\"\\n‚úÖ Tokenizer for '{MODEL_NAME}' loaded.\")\n","print(f\" Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\")\n","\n","\n","# ===============================\n","#  Helper Functions\n","# ===============================\n","\n","def create_dataset_dict(dialect_col):\n","    \"\"\"Creates a preprocessed DatasetDict for a given dialect.\"\"\"\n","    print(f\"\\n--- Processing dialect: {dialect_col} ---\")\n","    df_clean = df[['Standard_Bangla', dialect_col]].dropna()\n","    df_clean = df_clean[\n","        (df_clean['Standard_Bangla'].apply(lambda x: isinstance(x, str) and x.strip() != \"\")) &\n","        (df_clean[dialect_col].apply(lambda x: isinstance(x, str) and x.strip() != \"\"))\n","    ]\n","    subset_df = df_clean.rename(columns={'Standard_Bangla': 'target', dialect_col: 'source'})\n","\n","    if len(subset_df) < 20: # Need enough data to split\n","        print(f\"‚ö†Ô∏è Insufficient data for {dialect_col}. Skipping.\")\n","        return None\n","\n","    hf_dataset = Dataset.from_pandas(subset_df)\n","\n","    # Split, ensuring splits are not too small\n","    train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n","    test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n","\n","    dataset_dict = DatasetDict({\n","        'train': train_test_split['train'],\n","        'validation': test_val_split['train'],\n","        'test': test_val_split['test']\n","    })\n","    print(f\"‚úÖ Dataset splits created for {dialect_col}: Train {len(dataset_dict['train'])}, Val {len(dataset_dict['validation'])}, Test {len(dataset_dict['test'])}\")\n","    return dataset_dict\n","\n","\n","def tokenize_and_prepare_datasets(dataset_dict):\n","    \"\"\"Tokenizes the source and target text in the dataset.\"\"\"\n","\n","    def tokenize_fn(examples):\n","        # mBART tokenizer uses the `src_lang` set on the tokenizer\n","        model_inputs = tokenizer(\n","            examples[\"source\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        # mBART tokenizer uses `tgt_lang` when `text_target` is provided\n","        labels = tokenizer(\n","            text_target=examples[\"target\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    tokenized_datasets = dataset_dict.map(\n","        tokenize_fn,\n","        batched=True,\n","        remove_columns=['source', 'target'] # <-- Added remove_columns\n","    )\n","    print(\"‚úÖ Tokenization complete.\")\n","    return tokenized_datasets\n","\n","\n","def train_and_evaluate(dialect_name, train_ds, val_ds, test_ds):\n","    \"\"\"Initializes and runs the training, then evaluates on the test set.\"\"\"\n","\n","    # --- Memory Cleanup ---\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    # ----------------------\n","\n","    output_dir = f\"/content/Bangla_Dialect_Models/mbart-bangla-{dialect_name.lower()}\"\n","\n","    # --- Load fresh model ---\n","    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n","    # ------------------------\n","\n","    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","    bleu_metric = evaluate.load(\"sacrebleu\")\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = [pred.strip() for pred in decoded_preds]\n","        decoded_labels = [[label.strip()] for label in decoded_labels]\n","        result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","        return {\"bleu\": result[\"score\"]}\n","\n","    # --- Improved Training Arguments ---\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=2e-5,\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=2, # <-- Effective batch size = 8\n","\n","        per_device_eval_batch_size=8,  # <-- Can be larger for eval\n","        weight_decay=0.01,\n","        save_total_limit=2,\n","\n","        # Increased epochs to avoid underfitting\n","        num_train_epochs=5,           # <-- Set to 5\n","\n","        predict_with_generate=True,\n","        fp16=torch.cuda.is_available(),\n","\n","        # --- CRITICAL FIX: To save the best model ---\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"bleu\",\n","        # --------------------------------------------\n","\n","        # Your \"step\" settings\n","        eval_strategy=\"steps\",\n","        eval_steps=500,\n","        save_steps=500,     # Must match eval_steps\n","        logging_steps=100,\n","\n","        # Standard good practices\n","        warmup_steps=300,\n","        max_grad_norm=1.0,\n","        generation_max_length=128,\n","        generation_num_beams=4,\n","\n","        # Added to hide wandb logs\n","        report_to=\"none\",\n","    )\n","    # -----------------------------------\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=val_ds,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    print(f\"\\n Starting training for {dialect_name} dialect...\")\n","    trainer.train()\n","\n","    print(f\"\\n Evaluating on the test set for {dialect_name}...\")\n","    # --- Improved: Use original dataset for printing ---\n","    original_test_ds = split_dataset_dict[\"test\"]\n","\n","    test_results = trainer.predict(test_ds)\n","    final_bleu_score = test_results.metrics.get('test_bleu', 0.0)\n","    print(f\" Test Set BLEU Score for {dialect_name}: {final_bleu_score:.2f}\")\n","\n","    print(\"\\nüîç Example Translations:\")\n","    predictions = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n","    for i in range(min(5, len(predictions))):\n","        print(f\" Input (Dialect):   {original_test_ds[i]['source']}\")\n","        print(f\" Actual (Standard): {original_test_ds[i]['target']}\")\n","        print(f\" Predicted:         {predictions[i]}\\n\")\n","    # -------------------------------------------------\n","\n","    # Save the final (best) model\n","    trainer.save_model(f\"{output_dir}/best_model\")\n","    print(f\"‚úÖ Best model for {dialect_name} saved to {output_dir}/best_model\")\n","\n","    return trainer\n","\n","\n","# ===============================\n","# üöÄ Training Loop (Improved)\n","# ===============================\n","for dialect in DIALECTS_TO_TRAIN:\n","    split_dataset_dict = create_dataset_dict(dialect)\n","\n","    if split_dataset_dict is None:\n","        continue # Skip if dataset creation failed\n","\n","    tokenized_datasets = tokenize_and_prepare_datasets(split_dataset_dict)\n","\n","    # The train_and_evaluate function now handles model loading and saving\n","    trained_trainer = train_and_evaluate(\n","        dialect,\n","        tokenized_datasets[\"train\"],\n","        tokenized_datasets[\"validation\"],\n","        tokenized_datasets[\"test\"]\n","    )\n","\n","    print(f\"--- Finished processing {dialect} ---\")\n","\n","\n","print(\"\\n‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\")"],"metadata":{"id":"sOyPohhdF8Yi","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0ef24d00c634466aaf4aacdf7c41d308","174c68f50b604a0fa29e20db76eac6f8","48953b8df9144a90ac349b352262b4bd","8763a22545224ed8b18b9f0b61c28820","babdad96486347b2a66cc4e016d3aaec","5d8af6bf728a440096ad58bb4e80394a","90a76f0ccc944b2a830a1aba30f4b68b","5197d711f8f544daa5363b63d41e8c6b","b235feb338b54bc4a10bdccd4e081bd6","b97e33322cb6450992e7a14be50d77b6","6d13b665194442f2bd6473b2bdd2ed57","ab140085299f445ebd3356bb435cf3de","355b5af646494b74bce610ac2c2ddb80","0de0bfb146344f5888017c478d0a9349","8ded3b6baf8e442788150b8668ede73e","84fac6a2fc4c4ce795824577b658c268","b65b08c46cc94626a332a966e2e6452b","4c54e2fd2f2742bfa899cf53a87c8abf","33a52785e64b45e683a02582cd2c1521","bb27cdd356e04318a1ed6a1330ba51d6","6c3ffee89e9f4a15847610c7853a8ab5","9efc79a43dba4b52bb7f9a634d5d9fd9","90ed0e97785046d9b8697c8081b4839f","89da7744f9724e07b2e6f2cf694a400c","cf77a7634ee24de28afaa23dad831fd9","9a10e2cccf244c8097bfd87134bf6905","5623ddf8b35441559d93ff4c8d111f31","6b30e649ac5744a9a74be1e61ced820c","bc45d1da8fee4238a91b79f273926ecc","f3ec85ecbbc84c1d95e8cf80794e35da","d70a678ed1ca464aa112090f1e5b86de","2597fec443c1427081aa7389975eba22","32fdcdc7c6524301b131e2b9e034be87","76554ecde5474fea84214320e1499547","92270d19148848128b200066319cf519","e1a74c73d6bd4b7483a9d4fa92ec645e","b69d2e3a27c7441997791e4b21e07046","e868340458c94e1db01eb979ebf858a4","889b807732cc4c89abd99cced2ff09ec","e451406f3b564cfb83fdd3955c529026","7931a9928b5c4072af6a2f6ce9f665fc","84d361bbdb6041ccbea478e5fc61b6f4","339486a5c6884cc4ace6ebdd38545237","bc8ffb8300f34412aa063a3cd619d10e","96336c9daa4c40b289f66e98c48f135a","e16aef0a550840179208036d144b4977","4a33543a6508481faf6ca66ef07909c5","04a19eada4be416c9d9e27f08cf1d97a","3df7a20223084effbcc75deafe12b09a","52f09cf4555c40a4aea0ab2ceba3f355","d41b22731a8c46228afecc167e835f19","9b530b40a02e4a9397c92e0d6b3eabd8","df640154aeb1467ab9a9c7481e98b5bd","c1ecacd04a6d4cd88784750d9ccbdda2","824be4e9849e4fc6a773bd7fa4a4a166","6ba20531e8f54f559ea749bc563f46a9","f56a4c96bd5043c19b9d07f0d0cc111b","8ccb510569a64fc3865cd23ea624a29f","6eb2d9feb63d406896c80acf5d06f004","5549300fdd3a4b1ba73ba8b2e58652a2","2978e224b374487eb273f65ca79fad5c","9b25258d6d564ee1b7d7959a1ab966bb","575025814d0047959f9908712b86f57a","661a069f04c542b692b4a68e4a267da6","1d0ef165203348a2994c45df620550df","f6df57939b564353ba74d4cec4295915"]},"outputId":"0d762381-9ef4-4912-8d0d-a4085cc23360"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Libraries installed and imported successfully.\n","‚úÖ Successfully loaded the dataset\n","\n","‚úÖ Tokenizer for 'facebook/mbart-large-50-many-to-many-mmt' loaded.\n"," Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\n","\n","--- Processing dialect: Barisal ---\n","‚úÖ Dataset splits created for Barisal: Train 2784, Val 348, Test 348\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2784 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef24d00c634466aaf4aacdf7c41d308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab140085299f445ebd3356bb435cf3de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ed0e97785046d9b8697c8081b4839f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Tokenization complete.\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76554ecde5474fea84214320e1499547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96336c9daa4c40b289f66e98c48f135a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba20531e8f54f559ea749bc563f46a9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1373964637.py:191: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Starting training for Barisal dialect...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1740' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1740/1740 34:04, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.125700</td>\n","      <td>0.113813</td>\n","      <td>35.764739</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.060400</td>\n","      <td>0.089560</td>\n","      <td>44.350685</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.030300</td>\n","      <td>0.088473</td>\n","      <td>50.370624</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Evaluating on the test set for Barisal...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Test Set BLEU Score for Barisal: 47.85\n","\n","üîç Example Translations:\n"," Input (Dialect):   ‡¶Ü‡¶Æ‡ßç‡¶®‡ßá ‡¶ï‡¶ø ‡¶Æ‡ßã‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡¶õ‡¶®‡ßç‡¶¶‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n","\n"," Input (Dialect):   ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶Ç‡¶°‡¶æ ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶®\n"," Actual (Standard): ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶ô‡¶ü‡¶ø ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n"," Predicted:         ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶Ç‡¶ü‡¶æ ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n","\n"," Input (Dialect):   ‡¶õ‡ßã‡¶°‡ßã ‡¶¨‡ßÅ‡¶á‡¶® ‡¶ö‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶® ‡¶¶‡¶ø‡ßü‡¶æ ‡¶ì‡¶†‡¶õ‡ßá ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£\n"," Actual (Standard): ‡¶õ‡ßã‡¶ü‡ßã ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡ßé‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá ‡¶â‡¶†‡¶≤, ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£\n"," Predicted:         ‡¶õ‡ßã‡¶ü ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡ßé ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶†‡ßá‡¶õ‡ßá ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n","\n"," Input (Dialect):   ‡¶Ü‡¶Æ‡¶®‡ßá ‡¶ñ‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡ßá‡¶§‡¶õ‡ßá‡¶®\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡¶ø‡¶§‡¶≤‡ßá‡¶®\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ú‡¶ø‡¶§‡¶≤‡ßá‡¶®\n","\n"," Input (Dialect):   ‡¶π‡¶æ‡¶∞‡¶æ‡¶á‡¶Ø‡¶º‡¶æ ‡¶ó‡ßá‡¶õ‡¶ø\n"," Actual (Standard): ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n"," Predicted:         ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n","\n","‚úÖ Best model for Barisal saved to /content/Bangla_Dialect_Models/mbart-bangla-barisal/best_model\n","--- Finished processing Barisal ---\n","\n","‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\n"]}]},{"cell_type":"markdown","source":["## **BARISAL**"],"metadata":{"id":"zMEr8cND24Sa"}},{"cell_type":"code","source":["# ===============================\n","#  Bangla Dialect ‚Üí Standard Bangla using mBART-50 (Improved)\n","# ===============================\n","\n","!pip install transformers[sentencepiece] datasets sacrebleu evaluate torch pandas openpyxl --quiet\n","\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","import torch\n","import gc # <-- Added for memory cleanup\n","from google.colab import drive\n","\n","print(\"‚úÖ Libraries installed and imported successfully.\")\n","#drive.mount('/content/drive')\n","\n","# ===============================\n","#  Load Dataset\n","# ===============================\n","\n","try:\n","    df = pd.read_excel(\"/content/bangla_dialect_aligned_18920.xlsx\")\n","    print(\"‚úÖ Successfully loaded the dataset\")\n","except FileNotFoundError:\n","    print(\"‚ö†Ô∏è Dataset not found ‚Äî using sample data.\")\n","    df = pd.DataFrame({\n","        'Standard_Bangla': [\"‡¶∏‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Barisal': [\"‡¶π‡ßá‡¶á ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Chittagong': [\"‡¶π‡ßá‡¶á ‡¶∏‡ßç‡¶ï‡ßã‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Sylhet': [\"‡¶§‡¶æ‡¶∞‡ßá ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"]\n","    })\n","\n","# Choose dialects to train\n","DIALECTS_TO_TRAIN = ['Barisal'] # Can now train multiple, one after another\n","\n","# ===============================\n","#  Load Tokenizer (mBART)\n","# ===============================\n","\n","MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n","\n","# Load tokenizer once, it can be reused\n","# For mBART, we set the src/tgt lang on the tokenizer instance\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.src_lang = \"bn_IN\"\n","tokenizer.tgt_lang = \"bn_IN\"\n","\n","print(f\"\\n‚úÖ Tokenizer for '{MODEL_NAME}' loaded.\")\n","print(f\" Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\")\n","\n","\n","# ===============================\n","#  Helper Functions\n","# ===============================\n","\n","def create_dataset_dict(dialect_col):\n","    \"\"\"Creates a preprocessed DatasetDict for a given dialect.\"\"\"\n","    print(f\"\\n--- Processing dialect: {dialect_col} ---\")\n","    df_clean = df[['Standard_Bangla', dialect_col]].dropna()\n","    df_clean = df_clean[\n","        (df_clean['Standard_Bangla'].apply(lambda x: isinstance(x, str) and x.strip() != \"\")) &\n","        (df_clean[dialect_col].apply(lambda x: isinstance(x, str) and x.strip() != \"\"))\n","    ]\n","    subset_df = df_clean.rename(columns={'Standard_Bangla': 'target', dialect_col: 'source'})\n","\n","    if len(subset_df) < 20: # Need enough data to split\n","        print(f\"‚ö†Ô∏è Insufficient data for {dialect_col}. Skipping.\")\n","        return None\n","\n","    hf_dataset = Dataset.from_pandas(subset_df)\n","\n","    # Split, ensuring splits are not too small\n","    train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n","    test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n","\n","    dataset_dict = DatasetDict({\n","        'train': train_test_split['train'],\n","        'validation': test_val_split['train'],\n","        'test': test_val_split['test']\n","    })\n","    print(f\"‚úÖ Dataset splits created for {dialect_col}: Train {len(dataset_dict['train'])}, Val {len(dataset_dict['validation'])}, Test {len(dataset_dict['test'])}\")\n","    return dataset_dict\n","\n","\n","def tokenize_and_prepare_datasets(dataset_dict):\n","    \"\"\"Tokenizes the source and target text in the dataset.\"\"\"\n","\n","    def tokenize_fn(examples):\n","        # mBART tokenizer uses the `src_lang` set on the tokenizer\n","        model_inputs = tokenizer(\n","            examples[\"source\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        # mBART tokenizer uses `tgt_lang` when `text_target` is provided\n","        labels = tokenizer(\n","            text_target=examples[\"target\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    tokenized_datasets = dataset_dict.map(\n","        tokenize_fn,\n","        batched=True,\n","        remove_columns=['source', 'target'] # <-- Added remove_columns\n","    )\n","    print(\"‚úÖ Tokenization complete.\")\n","    return tokenized_datasets\n","\n","\n","def train_and_evaluate(dialect_name, train_ds, val_ds, test_ds):\n","    \"\"\"Initializes and runs the training, then evaluates on the test set.\"\"\"\n","\n","    # --- Memory Cleanup ---\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    # ----------------------\n","\n","    output_dir = f\"/content/Bangla_Dialect_Models/mbart-bangla-{dialect_name.lower()}\"\n","\n","    # --- Load fresh model ---\n","    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n","    # ------------------------\n","\n","    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","    bleu_metric = evaluate.load(\"sacrebleu\")\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = [pred.strip() for pred in decoded_preds]\n","        decoded_labels = [[label.strip()] for label in decoded_labels]\n","        result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","        return {\"bleu\": result[\"score\"]}\n","\n","    # --- Improved Training Arguments ---\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=2e-5,\n","\n","        # Use gradient accumulation for a larger effective batch size\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=2, # <-- Effective batch size = 8\n","\n","        per_device_eval_batch_size=8,  # <-- Can be larger for eval\n","        weight_decay=0.01,\n","        save_total_limit=2,\n","\n","        # Increased epochs to avoid underfitting\n","        num_train_epochs=5,           # <-- Set to 5\n","\n","        predict_with_generate=True,\n","        fp16=torch.cuda.is_available(),\n","\n","        # --- CRITICAL FIX: To save the best model ---\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"bleu\",\n","        # --------------------------------------------\n","\n","        # Your \"step\" settings\n","        eval_strategy=\"steps\",\n","        eval_steps=500,\n","        save_steps=500,     # Must match eval_steps\n","        logging_steps=100,\n","\n","        # Standard good practices\n","        warmup_steps=300,\n","        max_grad_norm=1.0,\n","        generation_max_length=128,\n","        generation_num_beams=4,\n","\n","        # Added to hide wandb logs\n","        report_to=\"none\",\n","    )\n","    # -----------------------------------\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=val_ds,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    print(f\"\\n Starting training for {dialect_name} dialect...\")\n","    trainer.train()\n","\n","    print(f\"\\n Evaluating on the test set for {dialect_name}...\")\n","    # --- Improved: Use original dataset for printing ---\n","    original_test_ds = split_dataset_dict[\"test\"]\n","\n","    test_results = trainer.predict(test_ds)\n","    final_bleu_score = test_results.metrics.get('test_bleu', 0.0)\n","    print(f\" Test Set BLEU Score for {dialect_name}: {final_bleu_score:.2f}\")\n","\n","    print(\"\\nüîç Example Translations:\")\n","    predictions = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n","    for i in range(min(5, len(predictions))):\n","        print(f\" Input (Dialect):   {original_test_ds[i]['source']}\")\n","        print(f\" Actual (Standard): {original_test_ds[i]['target']}\")\n","        print(f\" Predicted:         {predictions[i]}\\n\")\n","    # -------------------------------------------------\n","\n","    # Save the final (best) model\n","    trainer.save_model(f\"{output_dir}/best_model\")\n","    print(f\"‚úÖ Best model for {dialect_name} saved to {output_dir}/best_model\")\n","\n","    return trainer\n","\n","\n","# ===============================\n","# üöÄ Training Loop (Improved)\n","# ===============================\n","for dialect in DIALECTS_TO_TRAIN:\n","    split_dataset_dict = create_dataset_dict(dialect)\n","\n","    if split_dataset_dict is None:\n","        continue # Skip if dataset creation failed\n","\n","    tokenized_datasets = tokenize_and_prepare_datasets(split_dataset_dict)\n","\n","    # The train_and_evaluate function now handles model loading and saving\n","    trained_trainer = train_and_evaluate(\n","        dialect,\n","        tokenized_datasets[\"train\"],\n","        tokenized_datasets[\"validation\"],\n","        tokenized_datasets[\"test\"]\n","    )\n","\n","    print(f\"--- Finished processing {dialect} ---\")\n","\n","\n","print(\"\\n‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0ef24d00c634466aaf4aacdf7c41d308","174c68f50b604a0fa29e20db76eac6f8","48953b8df9144a90ac349b352262b4bd","8763a22545224ed8b18b9f0b61c28820","babdad96486347b2a66cc4e016d3aaec","5d8af6bf728a440096ad58bb4e80394a","90a76f0ccc944b2a830a1aba30f4b68b","5197d711f8f544daa5363b63d41e8c6b","b235feb338b54bc4a10bdccd4e081bd6","b97e33322cb6450992e7a14be50d77b6","6d13b665194442f2bd6473b2bdd2ed57","ab140085299f445ebd3356bb435cf3de","355b5af646494b74bce610ac2c2ddb80","0de0bfb146344f5888017c478d0a9349","8ded3b6baf8e442788150b8668ede73e","84fac6a2fc4c4ce795824577b658c268","b65b08c46cc94626a332a966e2e6452b","4c54e2fd2f2742bfa899cf53a87c8abf","33a52785e64b45e683a02582cd2c1521","bb27cdd356e04318a1ed6a1330ba51d6","6c3ffee89e9f4a15847610c7853a8ab5","9efc79a43dba4b52bb7f9a634d5d9fd9","90ed0e97785046d9b8697c8081b4839f","89da7744f9724e07b2e6f2cf694a400c","cf77a7634ee24de28afaa23dad831fd9","9a10e2cccf244c8097bfd87134bf6905","5623ddf8b35441559d93ff4c8d111f31","6b30e649ac5744a9a74be1e61ced820c","bc45d1da8fee4238a91b79f273926ecc","f3ec85ecbbc84c1d95e8cf80794e35da","d70a678ed1ca464aa112090f1e5b86de","2597fec443c1427081aa7389975eba22","32fdcdc7c6524301b131e2b9e034be87","76554ecde5474fea84214320e1499547","92270d19148848128b200066319cf519","e1a74c73d6bd4b7483a9d4fa92ec645e","b69d2e3a27c7441997791e4b21e07046","e868340458c94e1db01eb979ebf858a4","889b807732cc4c89abd99cced2ff09ec","e451406f3b564cfb83fdd3955c529026","7931a9928b5c4072af6a2f6ce9f665fc","84d361bbdb6041ccbea478e5fc61b6f4","339486a5c6884cc4ace6ebdd38545237","bc8ffb8300f34412aa063a3cd619d10e","96336c9daa4c40b289f66e98c48f135a","e16aef0a550840179208036d144b4977","4a33543a6508481faf6ca66ef07909c5","04a19eada4be416c9d9e27f08cf1d97a","3df7a20223084effbcc75deafe12b09a","52f09cf4555c40a4aea0ab2ceba3f355","d41b22731a8c46228afecc167e835f19","9b530b40a02e4a9397c92e0d6b3eabd8","df640154aeb1467ab9a9c7481e98b5bd","c1ecacd04a6d4cd88784750d9ccbdda2","824be4e9849e4fc6a773bd7fa4a4a166","6ba20531e8f54f559ea749bc563f46a9","f56a4c96bd5043c19b9d07f0d0cc111b","8ccb510569a64fc3865cd23ea624a29f","6eb2d9feb63d406896c80acf5d06f004","5549300fdd3a4b1ba73ba8b2e58652a2","2978e224b374487eb273f65ca79fad5c","9b25258d6d564ee1b7d7959a1ab966bb","575025814d0047959f9908712b86f57a","661a069f04c542b692b4a68e4a267da6","1d0ef165203348a2994c45df620550df","f6df57939b564353ba74d4cec4295915"]},"outputId":"0d762381-9ef4-4912-8d0d-a4085cc23360","collapsed":true,"id":"962N0VEx22Dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Libraries installed and imported successfully.\n","‚úÖ Successfully loaded the dataset\n","\n","‚úÖ Tokenizer for 'facebook/mbart-large-50-many-to-many-mmt' loaded.\n"," Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\n","\n","--- Processing dialect: Barisal ---\n","‚úÖ Dataset splits created for Barisal: Train 2784, Val 348, Test 348\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2784 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef24d00c634466aaf4aacdf7c41d308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab140085299f445ebd3356bb435cf3de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90ed0e97785046d9b8697c8081b4839f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Tokenization complete.\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76554ecde5474fea84214320e1499547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96336c9daa4c40b289f66e98c48f135a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba20531e8f54f559ea749bc563f46a9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1373964637.py:191: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Starting training for Barisal dialect...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1740' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1740/1740 34:04, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.125700</td>\n","      <td>0.113813</td>\n","      <td>35.764739</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.060400</td>\n","      <td>0.089560</td>\n","      <td>44.350685</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.030300</td>\n","      <td>0.088473</td>\n","      <td>50.370624</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Evaluating on the test set for Barisal...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Test Set BLEU Score for Barisal: 47.85\n","\n","üîç Example Translations:\n"," Input (Dialect):   ‡¶Ü‡¶Æ‡ßç‡¶®‡ßá ‡¶ï‡¶ø ‡¶Æ‡ßã‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡¶õ‡¶®‡ßç‡¶¶‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n","\n"," Input (Dialect):   ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶Ç‡¶°‡¶æ ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶®\n"," Actual (Standard): ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶ô‡¶ü‡¶ø ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n"," Predicted:         ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶Ç‡¶ü‡¶æ ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n","\n"," Input (Dialect):   ‡¶õ‡ßã‡¶°‡ßã ‡¶¨‡ßÅ‡¶á‡¶® ‡¶ö‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶® ‡¶¶‡¶ø‡ßü‡¶æ ‡¶ì‡¶†‡¶õ‡ßá ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£\n"," Actual (Standard): ‡¶õ‡ßã‡¶ü‡ßã ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡ßé‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá ‡¶â‡¶†‡¶≤, ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£\n"," Predicted:         ‡¶õ‡ßã‡¶ü ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡ßé ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶†‡ßá‡¶õ‡ßá ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n","\n"," Input (Dialect):   ‡¶Ü‡¶Æ‡¶®‡ßá ‡¶ñ‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡ßá‡¶§‡¶õ‡ßá‡¶®\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡¶ø‡¶§‡¶≤‡ßá‡¶®\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ú‡¶ø‡¶§‡¶≤‡ßá‡¶®\n","\n"," Input (Dialect):   ‡¶π‡¶æ‡¶∞‡¶æ‡¶á‡¶Ø‡¶º‡¶æ ‡¶ó‡ßá‡¶õ‡¶ø\n"," Actual (Standard): ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n"," Predicted:         ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n","\n","‚úÖ Best model for Barisal saved to /content/Bangla_Dialect_Models/mbart-bangla-barisal/best_model\n","--- Finished processing Barisal ---\n","\n","‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\n"]}]},{"cell_type":"markdown","source":["## **SYLHET**\n"],"metadata":{"id":"9echTGz74TQN"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7825b2e26ebb47e280230d3fa8698936","f65a09d3f845441f8650e14f491044bb","ee2453e3b3cd472fb1ee29950adb80bf","c094dbe321b74ed782e7eabf989915c6","d16d65c1d3714674a6459cc9464a7c29","cba1c7fd7b464540b94f6f60528c0188","e11d3267c5dd420aa8a79785a28fca27","0e8bd41c5aa34d63a50fc475d54184ef","a1fb48b8764544d6b8fb05357112a35c","e346088a473b4741908b1e08229221a6","4f1a3cc14d24425db4c5d3b4d52329a7","29604c11d7974f6c83fc54a93c1834b1","a43a31cc40454b0a9f237ae8a0354e84","99c8433c5e6a4793b1b0cdf798e04b0e","151e1a3327934037a9e6893f9f134efe","3c8ba756be70499eb0860d2bbd1edb0d","3f586cdba24c42528416bf7b21e7b2cb","59759da4ba9f43a3b455955fe5c7c774","72c902cae1c44b56b14d9cbcf6b1e9a7","2d49a45d233c4c33add1a6a30a214dc0","4752b69e93aa4f35883038b38ea9abba","59bea4b06f7c4840a59e032fc071f31b","5d4a81f3e7594bcd963d11c800c80b80","c5abb6a3bace47b6aae0c62802ef462f","37bb63e61bdf497c9b5f8d7a3113112c","e181d26b5cef4824a4ee14c47a57d1f5","5e5cb3af8ae64dfeb4edbd5c785c8d1f","41c76d7cf0064e909f6de735e4de197c","8a793c12ebd44c8e95f0156cd3420b26","913ec135718544ad975e0e4bc5a42dc5","7224a1538b9d4a969c005ff925859293","8481dab27b1448659b46bdbc71377638","ca55241275494915aa95a189d9b4e74c","241b1cae0874468ca6978ade5d0b3193","643a09a84d674402a317e8cbb74db483","c27a53cb815b4fcf98976bbe07a69203","2ffb8e949a40426f97ae6717a974fa13","29b1faac2b10409aa1c961e259f741ed","3ce32544e6b2448fb630124e4e03af09","e75cf2f63c744718813e336251d8a1ef","38d8109720af4300aa68f61efbfb1541","46819fa9bd0944848e7ca49d4193d89e","b77ea5f2bde942c5b05f2362612a96fa","a287444bcc474e218bd9c405732a45ea","56d7f9b1ca614868904e1dd37a251005","ba0bd44514594717a1eb15decc29534d","36c4d2bcacf942db85ef9383d2c23bb3","73f061e2bbfb40d392d2eccc5ba4dba7","3d114e78aeb1455495cb3336ff991466","4713e25db19040c1a8013ee78655b0cb","e876c2c7765c42079ffca74a11698d0b","50fa16caa2cb4d4f89e56bda364d7a2e","bcf469676358420fbef6d3f6c9eb9439","a3049503613d44c89ac40f2135ee6fbf","58cb5693d70043298e69f64043a99302","4f02bb9ffc42409298aea529a7f94795","f6ea4291a4744d078d326487251fe95d","685cb74f193f48648cc01de17eced524","2794817de0b14406af6d14ba1fdd3ad2","49f0440796f142f5a7a1be17ffa4de3f","b33be9df10014cd392bf86f2bbcae22c","fc0fe0414a5741bdbb0a2e5e3c8d97de","626d190ff1764b40a6fe1080b625de71","fe30149821f548baa29d700b491f1d40","8a022ed828834c898e8b29f74308dbda","2ee6ed409afa49419f21b8d394f4bfea","69d816a9ae914faca9ac98ae3fab36e2","38f7d1e2cb0d42858f25331095e8b64e","19c1142776b44e72ba244a5e32796a4a","f35c16f764d5413d8745dc5ed09d910d","50d31d69be764f52b22e3807d60eac08","d8e1285582194148a6f7d33bab60faa2","a03d0604d3c74f9d981d2664be7a4f1c","24a2daff6ad4429081cd2fbe08ac42f5","1c811bbded7740ddbfd8dccbe2a96f77","20abca4fa4274e1881e5be9981648ecc","1afdb16868914546af71e5062618953a","486fce906d0c46278adf8ac1a4bb9810","227ad61ec8844e5db95f0cf7191ee90e","9343be34d75b4ca59dce46c4c37af6e0","fc0c02dbc9724902be54b0b6af0a4097","d5a8527f6f1c47dbbd366d59447a07b5","157269f2927d4e48877df7e20c5a9d8e","3b79f39104734e948ff328f3c421b768","797111f8fc6d4f309a2829b7c714ffd5","00209c392b0148b197563bdfd0b098eb","f6fe88cba95949c48e51a7e1ba371d21","230b535604a94938bdea65d52b32b320","4b75f2b53b344081ae1f6dc3a932db6d","0d5e35c9b3904fa68bc829b65ae9a6c2","91d97d892c98431790991828f4907871","daf3d5cc1fe14af7a86af95532a40d13","9f355d7c50794076a81c59372dd588f4","4057bfe79a244c26bc8ce92083cfac76","1a509ff8d3464e1ea715076343414b15","5973e75329a34c66a80fcdd323f99c30","0508e256cc804b0d9c4a6e12191e3b3c","e4576339377a4f00ac11502d09639291","bd09aeb86b944af18c594bdcba46943a","4e91f35d4e3c4bb2847107a682d20bb1","1c32b5b835b44a40846a6428594024e2","a57fbf34d3f94f86a554ae6e676e8414","d9f53fc83efc40a0b64156dcf2f77bee","9de50abbe23946ef941ce1f6ed197099","53142f8f9aa242d9b6ab9f1570501422","b3a9ab5fa31f4c0f9da4f10f81c4db0a","7febacce23d04f9b8ef9e4a0dd90ae33","36df4ca93f324bbf89e64467af63075b","f181796292944ad4b14b596f0cccefa2","e42c75657a2b43e1b4b1ed413f8cac15"]},"id":"c4GeosoRUgLb","outputId":"e496f320-0ddc-431c-e808-ba8503c86d5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h‚úÖ Libraries installed and imported successfully.\n","‚úÖ Successfully loaded the dataset\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7825b2e26ebb47e280230d3fa8698936"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29604c11d7974f6c83fc54a93c1834b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d4a81f3e7594bcd963d11c800c80b80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241b1cae0874468ca6978ade5d0b3193"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Tokenizer for 'facebook/mbart-large-50-many-to-many-mmt' loaded.\n"," Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\n","\n","--- Processing dialect: Sylhet ---\n","‚úÖ Dataset splits created for Sylhet: Train 2784, Val 348, Test 348\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2784 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d7f9b1ca614868904e1dd37a251005"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f02bb9ffc42409298aea529a7f94795"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69d816a9ae914faca9ac98ae3fab36e2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Tokenization complete.\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"486fce906d0c46278adf8ac1a4bb9810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b75f2b53b344081ae1f6dc3a932db6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e91f35d4e3c4bb2847107a682d20bb1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3349345801.py:191: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Starting training for Sylhet dialect...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1740' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1740/1740 47:18, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.138400</td>\n","      <td>0.131626</td>\n","      <td>28.566290</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.072100</td>\n","      <td>0.108993</td>\n","      <td>37.558164</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.035700</td>\n","      <td>0.108468</td>\n","      <td>41.329051</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Evaluating on the test set for Sylhet...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Test Set BLEU Score for Sylhet: 38.55\n","\n","üîç Example Translations:\n"," Input (Dialect):   ‡¶Ü‡¶´‡¶®‡ßá ‡¶ï‡¶ø‡¶§‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®'‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶´‡¶æ‡¶∞‡¶¨‡¶æ ‡¶®‡¶ø?\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶®‡ßç‡¶ß‡ßÅ‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶®?\n","\n"," Input (Dialect):   ‡¶Ü‡¶ï‡¶æ‡¶∂ ‡¶ì‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶ô‡¶ü‡¶æ ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n"," Actual (Standard): ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶ô‡¶ü‡¶ø ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n"," Predicted:         ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶ô‡¶ü‡¶æ ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n","\n"," Input (Dialect):   ‡¶õ‡ßã‡¶ü ‡¶¨‡¶á‡¶®‡ßá ‡¶ö‡¶ø‡¶≤‡ßç‡¶≤‡¶æ‡¶á‡ßü‡¶æ ‡¶â‡¶†‡¶≤‡ßã, ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£\n"," Actual (Standard): ‡¶õ‡ßã‡¶ü‡ßã ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡ßé‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá ‡¶â‡¶†‡¶≤, ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£\n"," Predicted:         ‡¶õ‡ßã‡¶ü‡ßã ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶†‡¶≤‡ßã,‡¶¶‡¶æ‡¶∞‡ßÅ‡¶®\n","\n"," Input (Dialect):   ‡¶Ü‡¶´‡¶®‡ßá ‡¶ñ‡ßá‡¶≤‡¶æ‡¶§ ‡¶ú‡¶ø‡¶§‡¶≤‡¶æ\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡¶ø‡¶§‡¶≤‡ßá‡¶®\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ú‡¶ø‡¶§‡¶≤‡ßá‡¶®\n","\n"," Input (Dialect):   ‡¶Ü‡¶Æ‡¶ø ‡¶π‡¶æ‡¶∞‡¶ø ‡¶ó‡ßá‡¶õ‡¶ø\n"," Actual (Standard): ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n"," Predicted:         ‡¶Ü‡¶Æ‡¶ø ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n","\n"]}],"source":["# ===============================\n","#  Bangla Dialect ‚Üí Standard Bangla using mBART-50 (Improved)\n","# ===============================\n","\n","!pip install transformers[sentencepiece] datasets sacrebleu evaluate torch pandas openpyxl --quiet\n","\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","import torch\n","import gc # <-- Added for memory cleanup\n","from google.colab import drive\n","\n","print(\"‚úÖ Libraries installed and imported successfully.\")\n","#drive.mount('/content/drive')\n","\n","# ===============================\n","#  Load Dataset\n","# ===============================\n","\n","try:\n","    df = pd.read_excel(\"/content/bangla_dialect_aligned_18920.xlsx\")\n","    print(\"‚úÖ Successfully loaded the dataset\")\n","except FileNotFoundError:\n","    print(\"‚ö†Ô∏è Dataset not found ‚Äî using sample data.\")\n","    df = pd.DataFrame({\n","        'Standard_Bangla': [\"‡¶∏‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Barisal': [\"‡¶π‡ßá‡¶á ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Chittagong': [\"‡¶π‡ßá‡¶á ‡¶∏‡ßç‡¶ï‡ßã‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Sylhet': [\"‡¶§‡¶æ‡¶∞‡ßá ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"]\n","    })\n","\n","# Choose dialects to train\n","DIALECTS_TO_TRAIN = ['Sylhet'] # Can now train multiple, one after another\n","\n","# ===============================\n","#  Load Tokenizer (mBART)\n","# ===============================\n","\n","MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n","\n","# Load tokenizer once, it can be reused\n","# For mBART, we set the src/tgt lang on the tokenizer instance\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.src_lang = \"bn_IN\"\n","tokenizer.tgt_lang = \"bn_IN\"\n","\n","print(f\"\\n‚úÖ Tokenizer for '{MODEL_NAME}' loaded.\")\n","print(f\" Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\")\n","\n","\n","# ===============================\n","#  Helper Functions\n","# ===============================\n","\n","def create_dataset_dict(dialect_col):\n","    \"\"\"Creates a preprocessed DatasetDict for a given dialect.\"\"\"\n","    print(f\"\\n--- Processing dialect: {dialect_col} ---\")\n","    df_clean = df[['Standard_Bangla', dialect_col]].dropna()\n","    df_clean = df_clean[\n","        (df_clean['Standard_Bangla'].apply(lambda x: isinstance(x, str) and x.strip() != \"\")) &\n","        (df_clean[dialect_col].apply(lambda x: isinstance(x, str) and x.strip() != \"\"))\n","    ]\n","    subset_df = df_clean.rename(columns={'Standard_Bangla': 'target', dialect_col: 'source'})\n","\n","    if len(subset_df) < 20: # Need enough data to split\n","        print(f\"‚ö†Ô∏è Insufficient data for {dialect_col}. Skipping.\")\n","        return None\n","\n","    hf_dataset = Dataset.from_pandas(subset_df)\n","\n","    # Split, ensuring splits are not too small\n","    train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n","    test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n","\n","    dataset_dict = DatasetDict({\n","        'train': train_test_split['train'],\n","        'validation': test_val_split['train'],\n","        'test': test_val_split['test']\n","    })\n","    print(f\"‚úÖ Dataset splits created for {dialect_col}: Train {len(dataset_dict['train'])}, Val {len(dataset_dict['validation'])}, Test {len(dataset_dict['test'])}\")\n","    return dataset_dict\n","\n","\n","def tokenize_and_prepare_datasets(dataset_dict):\n","    \"\"\"Tokenizes the source and target text in the dataset.\"\"\"\n","\n","    def tokenize_fn(examples):\n","        # mBART tokenizer uses the `src_lang` set on the tokenizer\n","        model_inputs = tokenizer(\n","            examples[\"source\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        # mBART tokenizer uses `tgt_lang` when `text_target` is provided\n","        labels = tokenizer(\n","            text_target=examples[\"target\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    tokenized_datasets = dataset_dict.map(\n","        tokenize_fn,\n","        batched=True,\n","        remove_columns=['source', 'target'] # <-- Added remove_columns\n","    )\n","    print(\"‚úÖ Tokenization complete.\") # Corrected indentation\n","    return tokenized_datasets\n","\n","\n","def train_and_evaluate(dialect_name, train_ds, val_ds, test_ds):\n","    \"\"\"Initializes and runs the training, then evaluates on the test set.\"\"\"\n","\n","    # --- Memory Cleanup ---\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    # ----------------------\n","\n","    output_dir = f\"/content/Bangla_Dialect_Models/mbart-bangla-{dialect_name.lower()}\"\n","\n","    # --- Load fresh model ---\n","    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n","    # ------------------------\n","\n","    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","    bleu_metric = evaluate.load(\"sacrebleu\")\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = [pred.strip() for pred in decoded_preds]\n","        decoded_labels = [[label.strip()] for label in decoded_labels]\n","        result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","        return {\"bleu\": result[\"score\"]}\n","\n","    # --- Improved Training Arguments ---\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=2e-5,\n","\n","        # Use gradient accumulation for a larger effective batch size\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=2, # <-- Effective batch size = 8\n","\n","        per_device_eval_batch_size=8,  # <-- Can be larger for eval\n","        weight_decay=0.01,\n","        save_total_limit=2,\n","\n","        # Increased epochs to avoid underfitting\n","        num_train_epochs=5,           # <-- Set to 5\n","\n","        predict_with_generate=True,\n","        fp16=torch.cuda.is_available(),\n","\n","        # --- CRITICAL FIX: To save the best model ---\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"bleu\",\n","        # --------------------------------------------\n","\n","        # Your \"step\" settings\n","        eval_strategy=\"steps\",\n","        eval_steps=500,\n","        save_steps=500,     # Must match eval_steps\n","        logging_steps=100,\n","\n","        # Standard good practices\n","        warmup_steps=300,\n","        max_grad_norm=1.0,\n","        generation_max_length=128,\n","        generation_num_beams=4,\n","\n","        # Added to hide wandb logs\n","        report_to=\"none\",\n","    )\n","    # -----------------------------------\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=val_ds,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    print(f\"\\n Starting training for {dialect_name} dialect...\")\n","    trainer.train()\n","\n","    print(f\"\\n Evaluating on the test set for {dialect_name}...\")\n","    # --- Improved: Use original dataset for printing ---\n","    original_test_ds = split_dataset_dict[\"test\"]\n","\n","    test_results = trainer.predict(test_ds)\n","    final_bleu_score = test_results.metrics.get('test_bleu', 0.0)\n","    print(f\" Test Set BLEU Score for {dialect_name}: {final_bleu_score:.2f}\")\n","\n","    print(\"\\nüîç Example Translations:\")\n","    predictions = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n","    for i in range(min(5, len(predictions))):\n","        print(f\" Input (Dialect):   {original_test_ds[i]['source']}\")\n","        print(f\" Actual (Standard): {original_test_ds[i]['target']}\")\n","        print(f\" Predicted:         {predictions[i]}\\n\")\n","    # -------------------------------------------------\n","\n","    # Save the final (best) model\n","    trainer.save_model(f\"{output_dir}/best_model\")\n","    print(f\"‚úÖ Best model for {dialect_name} saved to {output_dir}/best_model\")\n","\n","    return trainer\n","\n","\n","# ===============================\n","# üöÄ Training Loop (Improved)\n","# ===============================\n","for dialect in DIALECTS_TO_TRAIN:\n","    split_dataset_dict = create_dataset_dict(dialect)\n","\n","    if split_dataset_dict is None:\n","        continue # Skip if dataset creation failed\n","\n","    tokenized_datasets = tokenize_and_prepare_datasets(split_dataset_dict)\n","\n","    # The train_and_evaluate function now handles model loading and saving\n","    trained_trainer = train_and_evaluate(\n","        dialect,\n","        tokenized_datasets[\"train\"],\n","        tokenized_datasets[\"validation\"],\n","        tokenized_datasets[\"test\"]\n","    )\n","\n","    print(f\"--- Finished processing {dialect} ---\")\n","\n","\n","print(\"\\n‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\")"]},{"cell_type":"markdown","source":["## **CHITTAGONG**"],"metadata":{"id":"DeOBAkOM4tnz"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6ef6f34009f447098c0a7d67ef573798","c6a9cfa12a344263a75ee4a94e118775","f7b4d94c482740de9d0538454393a083","98e6f2b167494a56a4ee912cc63db080","41e0b5e15763432cab7f1303d0d998e6","1d21e9fd9a3d497f94540292b19da332","0efca82251914efe953b6225b2ebb483","c70c4d7c8fea4678be61a7e28ad2713f","6584bb8c6c6349e4b751854eef1e0de5","dd6e368ed5d9407797d9effb784b997c","2e9255a66f1943148645499b01d8645b","ba1451f2345c445d8f617361cc00e01b","523c44b0a5ab4fa293b28cc33c914802","d274529e46f2468ebfbbc9bd35a737ae","04a49e05c95b47c5b3d1d598b8c1d44d","bc2b151bb3cb435d80ef08fc497378c9","8a9a8093977c47e0b4a932ff45d9cf43","ef484ccbb489464dbb9e0bb150de3bb0","b7e5b8dc175c4334b4ff1d2a4f4c4680","ca61f08a501146e9a74cd1bb16c9b1a5","bccc846a872045449300f1686194deee","a0f79de0cc8948be8abd0011aca43039","16071d34c5ea4f92b5e719dee522c9e4","0aa1aa4243c3492db998c39915f04e20","238053c4a2ec46bf95c70e58b8e56d7a","7e6e3c10da2a406facb95dfc8d860bee","a24c35396967496b9027b8b738c62067","dd8d15822c0e484ebb5a6d5c84468b5d","af43f8cfce0e47ec9760a54b76381e5a","9dd28b88cbab490bb781d5439a994202","a0eff487b235452c87ed4986d4ecdf3d","7383ce0ba90c4ea7960f2ce2dcd27958","3b522fadbe164110ab7aba8c02e49efe","2e0025bf9a65462baff81e29fe523b24","d726102dea1d4b16af5e2c294c24bc64","a06adef03fa948799c3d09e4bd46295b","d929a8657d6b4470a703ff1304751811","493a7db38e8647fb88e6bd3baecf8f1c","e33022ff837f4bef81e77bbf308755aa","61e48d463b1749109a5cb659c8c23d20","2eb0b15cd3b04a32958c8491afc26289","d02491872cce43588ff29a22d001b62e","de613a461b6c4941b368ab661cb62ccb","70a79cb4b48e4325b91f8c189d17ec21","3ccc128537d0430ba3c7751a89122d84","09e19fd057104b8880c3018b886f9b6f","635015d0b04c4fc8b0185c9b0b84f69a","ce3c1c8be3344360b19bc30dcf105fc8","2de1510c59c54b168847048328b00cc8","83af80f1b3be49499390d40ba337e02e","6156d3d2279b4dacab9ccd047b8bfd61","893e2a8725cd4cc69bea0aff27aa0363","ea007fc9aa314cb4933019cc6f09bb55","f9d54d261ca54110aeb677b0a4b4dc90","1507d1a951814abca4a2b410dc5e2218","55d345d8e8be4a0d99516d06deef8af1","752d24a9ada944c288d102c57a6feda9","ec89b20d5ab2449eb79d5f5a0053fa72","3d0b5ca6074e40569c2e22069963dc66","aa98c504a0a549748a066b927004049a","eafb1ef6caae42c39e6dac8e6a08940e","76d3556038f9403a98822407c664a1da","a550df6a2d57498f908234359916a0b0","28823c8e45ba46b39904b251f85e04f3","aa8ed1f941b74bda9e4b20c9693d040b","049f99b7833e4ea1b803c12f666515b0","950cd96a9ccb4892ace0e441613831c8","2891cc77a5694eecb0c96b5b1d50efc0","a260ad832049432a86248e73395f4785","44261a5e2ed647d09750635bdcc2f016","2e3f27f932554a4e93b2c9e16fba70ef","6bf75cd8abb74a198432ef85511fc538","0697bd40eff949849be85897bc1d7477","5609b9ce6b8649b0b11348e9fb5cd099","06db146c58f744c6b02dbc13b0f883f7","b1ee03fc553b4a3081114e3574769ad4","bc9136cbb460452fb1996b1db9265d1a","945546841e154f8c903d8ddbf8ce7b13","749f5888471045ed9df309c9f9ea349d","d9c6389ef6d5476295fdd8f8147a7132","dcf8c817b72147d88001a5f84e54b2d5","7fa982c7e250472aa9d70f7f90b85cad","2cf76e31a2754632881b10ce21125e24","a7cb4862024248638f573421646fe47a","72623749a1e4438b8a37b49e694398be","7746502c0cc44570b499b90f0c209fa2","4bfd9d4bd76041f4b87655bf6963e12d","c0d1aaeaa2844c6bb32a4a3ad4000e30","165fa9b0baab46bd9a56a64af54ec6e4","e190a748908e4bc692597e8558bcdf05","f9d2b05568ff4025a07f8fa83144b4d4","f79c32a52ca046f1b6e27a699c96a88c","2b0841954e814b2cb4ed50350b089011","acca56c476f94a4ba2743496cb317663","3a4de170d04f4c7eb0e65c1e6eed01b1","58dff332a484487893f30dba3f7d2114","9aa1f6e9f39942789052457959b03506","382e82323a8347f1895cedd2afb8cdd6","57f1daf298f2453f943f4084516f1ef0","0e3567cee19146b3bcaed06877250eb1","525778096ecb4a959ea80849b96b6447","aaf460c7a7e64ba699ac47edc3b2a908","f3b5424eff6f4830aab9db1ab3def31b","50d02843d4684bb094f60ed63f18ffa6","7907b4fa96c847618afd4aeac430f877","fd507322d1b54d1192e026bdc0393879","3cad9046ebc4491281742d464ee246fc","b5300d1227e9442da22ba1af9f7406aa","e968dbcc81f340e09f9920a8fad78a7d","5ea3203279bd4a75b2c8a6bd46ff59bf"]},"id":"EFYscdyugpKB","outputId":"f4890f77-5c2a-4c81-fb35-c24269c7a551"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h‚úÖ Libraries installed and imported successfully.\n","‚úÖ Successfully loaded the dataset\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef6f34009f447098c0a7d67ef573798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba1451f2345c445d8f617361cc00e01b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16071d34c5ea4f92b5e719dee522c9e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e0025bf9a65462baff81e29fe523b24"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Tokenizer for 'facebook/mbart-large-50-many-to-many-mmt' loaded.\n"," Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\n","\n","--- Processing dialect: Chittagong ---\n","‚úÖ Dataset splits created for Chittagong: Train 2784, Val 348, Test 348\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2784 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ccc128537d0430ba3c7751a89122d84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d345d8e8be4a0d99516d06deef8af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/348 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"950cd96a9ccb4892ace0e441613831c8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Tokenization complete.\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"945546841e154f8c903d8ddbf8ce7b13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165fa9b0baab46bd9a56a64af54ec6e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3567cee19146b3bcaed06877250eb1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4163874506.py:191: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Starting training for Chittagong dialect...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1740' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1740/1740 45:10, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.172700</td>\n","      <td>0.166116</td>\n","      <td>20.532833</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.090700</td>\n","      <td>0.136891</td>\n","      <td>32.607105</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.046200</td>\n","      <td>0.139361</td>\n","      <td>33.216564</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Evaluating on the test set for Chittagong...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Test Set BLEU Score for Chittagong: 30.98\n","\n","üîç Example Translations:\n"," Input (Dialect):   ‡¶Ö‡¶®‡ßá ‡¶ï‡¶ø ‡¶Ü‡¶Å‡¶∞ ‡¶´‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§ ‡¶´‡¶æ‡¶∞‡¶ø‡¶¨‡¶æ‡¶® ‡¶®‡ßá\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶™‡¶õ‡¶®‡ßç‡¶¶‡ßá‡¶∞ ‡¶ú‡¶¨‡¶æ‡¶¨ ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®?\n","\n"," Input (Dialect):   ‡¶Ü‡¶∏‡ßç‡¶∏‡¶æ‡¶® ‡¶Ö‡¶∞ ‡¶®‡¶ø‡¶≤ ‡¶∞‡¶Ç ‡¶á‡¶¨‡¶æ ‡¶Ö‡¶∏‡¶æ‡¶¶‡¶æ‡¶∞‡¶®\n"," Actual (Standard): ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶ô‡¶ü‡¶ø ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n"," Predicted:         ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶®‡ßÄ‡¶≤ ‡¶∞‡¶ô‡¶ü‡¶ø ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£\n","\n"," Input (Dialect):   ‡¶õ‡ßã‡ßã‡¶° ‡¶≠‡ßã‡¶á‡¶® ‡¶ö‡¶ø‡ßé‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá‡¶∞‡ßá ‡¶â‡¶°‡¶ø ‡¶ó‡¶ø‡ßü‡ßá\n"," Actual (Standard): ‡¶õ‡ßã‡¶ü‡ßã ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡ßé‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá ‡¶â‡¶†‡¶≤, ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£\n"," Predicted:         ‡¶õ‡ßã‡¶ü ‡¶¨‡ßã‡¶® ‡¶ö‡¶ø‡ßé‡¶ï‡¶æ‡¶∞ ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶â‡¶†‡ßá ‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤\n","\n"," Input (Dialect):   ‡¶ì‡¶®‡ßá ‡¶π‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡¶ø‡¶§‡ßç‡¶§‡¶®\n"," Actual (Standard): ‡¶Ü‡¶™‡¶®‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ‡ßü ‡¶ú‡¶ø‡¶§‡¶≤‡ßá‡¶®\n"," Predicted:         ‡¶Ü‡¶™‡¶®‡¶ø ‡¶π‡ßá‡¶∞‡ßá ‡¶â‡¶†‡ßÅ‡¶®\n","\n"," Input (Dialect):   ‡¶π‡¶æ‡¶∞‡ßÄ ‡¶ó‡¶ø‡¶Ø‡¶º‡¶ø\n"," Actual (Standard): ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n"," Predicted:         ‡¶π‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶õ‡¶ø\n","\n","‚úÖ Best model for Chittagong saved to /content/Bangla_Dialect_Models/mbart-bangla-chittagong/best_model\n","--- Finished processing Chittagong ---\n","\n","‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\n"]}],"source":["# ===============================\n","#  Bangla Dialect ‚Üí Standard Bangla using mBART-50 (Improved)\n","# ===============================\n","\n","!pip install transformers[sentencepiece] datasets sacrebleu evaluate torch pandas openpyxl --quiet\n","\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","import torch\n","import gc # <-- Added for memory cleanup\n","from google.colab import drive\n","\n","print(\"‚úÖ Libraries installed and imported successfully.\")\n","#drive.mount('/content/drive')\n","\n","# ===============================\n","#  Load Dataset\n","# ===============================\n","\n","try:\n","    df = pd.read_excel(\"/content/bangla_dialect_aligned_18920.xlsx\")\n","    print(\"‚úÖ Successfully loaded the dataset\")\n","except FileNotFoundError:\n","    print(\"‚ö†Ô∏è Dataset not found ‚Äî using sample data.\")\n","    df = pd.DataFrame({\n","        'Standard_Bangla': [\"‡¶∏‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Barisal': [\"‡¶π‡ßá‡¶á ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Chittagong': [\"‡¶π‡ßá‡¶á ‡¶∏‡ßç‡¶ï‡ßã‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Sylhet': [\"‡¶§‡¶æ‡¶∞‡ßá ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"]\n","    })\n","\n","# Choose dialects to train\n","DIALECTS_TO_TRAIN = ['Chittagong'] # Can now train multiple, one after another\n","\n","# ===============================\n","#  Load Tokenizer (mBART)\n","# ===============================\n","\n","MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n","\n","# Load tokenizer once, it can be reused\n","# For mBART, we set the src/tgt lang on the tokenizer instance\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.src_lang = \"bn_IN\"\n","tokenizer.tgt_lang = \"bn_IN\"\n","\n","print(f\"\\n‚úÖ Tokenizer for '{MODEL_NAME}' loaded.\")\n","print(f\" Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\")\n","\n","\n","# ===============================\n","#  Helper Functions\n","# ===============================\n","\n","def create_dataset_dict(dialect_col):\n","    \"\"\"Creates a preprocessed DatasetDict for a given dialect.\"\"\"\n","    print(f\"\\n--- Processing dialect: {dialect_col} ---\")\n","    df_clean = df[['Standard_Bangla', dialect_col]].dropna()\n","    df_clean = df_clean[\n","        (df_clean['Standard_Bangla'].apply(lambda x: isinstance(x, str) and x.strip() != \"\")) &\n","        (df_clean[dialect_col].apply(lambda x: isinstance(x, str) and x.strip() != \"\"))\n","    ]\n","    subset_df = df_clean.rename(columns={'Standard_Bangla': 'target', dialect_col: 'source'})\n","\n","    if len(subset_df) < 20: # Need enough data to split\n","        print(f\"‚ö†Ô∏è Insufficient data for {dialect_col}. Skipping.\")\n","        return None\n","\n","    hf_dataset = Dataset.from_pandas(subset_df)\n","\n","    # Split, ensuring splits are not too small\n","    train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n","    test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n","\n","    dataset_dict = DatasetDict({\n","        'train': train_test_split['train'],\n","        'validation': test_val_split['train'],\n","        'test': test_val_split['test']\n","    })\n","    print(f\"‚úÖ Dataset splits created for {dialect_col}: Train {len(dataset_dict['train'])}, Val {len(dataset_dict['validation'])}, Test {len(dataset_dict['test'])}\")\n","    return dataset_dict\n","\n","\n","def tokenize_and_prepare_datasets(dataset_dict):\n","    \"\"\"Tokenizes the source and target text in the dataset.\"\"\"\n","\n","    def tokenize_fn(examples):\n","        # mBART tokenizer uses the `src_lang` set on the tokenizer\n","        model_inputs = tokenizer(\n","            examples[\"source\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        # mBART tokenizer uses `tgt_lang` when `text_target` is provided\n","        labels = tokenizer(\n","            text_target=examples[\"target\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    tokenized_datasets = dataset_dict.map(\n","        tokenize_fn,\n","        batched=True,\n","        remove_columns=['source', 'target'] # <-- Added remove_columns\n","    )\n","    print(\"‚úÖ Tokenization complete.\") # Corrected indentation\n","    return tokenized_datasets\n","\n","\n","def train_and_evaluate(dialect_name, train_ds, val_ds, test_ds):\n","    \"\"\"Initializes and runs the training, then evaluates on the test set.\"\"\"\n","\n","    # --- Memory Cleanup ---\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    # ----------------------\n","\n","    output_dir = f\"/content/Bangla_Dialect_Models/mbart-bangla-{dialect_name.lower()}\"\n","\n","    # --- Load fresh model ---\n","    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n","    # ------------------------\n","\n","    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","    bleu_metric = evaluate.load(\"sacrebleu\")\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = [pred.strip() for pred in decoded_preds]\n","        decoded_labels = [[label.strip()] for label in decoded_labels]\n","        result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","        return {\"bleu\": result[\"score\"]}\n","\n","    # --- Improved Training Arguments ---\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=2e-5,\n","\n","        # Use gradient accumulation for a larger effective batch size\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=2, # <-- Effective batch size = 8\n","\n","        per_device_eval_batch_size=8,  # <-- Can be larger for eval\n","        weight_decay=0.01,\n","        save_total_limit=2,\n","\n","        # Increased epochs to avoid underfitting\n","        num_train_epochs=5,           # <-- Set to 5\n","\n","        predict_with_generate=True,\n","        fp16=torch.cuda.is_available(),\n","\n","        # --- CRITICAL FIX: To save the best model ---\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"bleu\",\n","        # --------------------------------------------\n","\n","        # Your \"step\" settings\n","        eval_strategy=\"steps\",\n","        eval_steps=500,\n","        save_steps=500,     # Must match eval_steps\n","        logging_steps=100,\n","\n","        # Standard good practices\n","        warmup_steps=300,\n","        max_grad_norm=1.0,\n","        generation_max_length=128,\n","        generation_num_beams=4,\n","\n","        # Added to hide wandb logs\n","        report_to=\"none\",\n","    )\n","    # -----------------------------------\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=val_ds,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    print(f\"\\n Starting training for {dialect_name} dialect...\")\n","    trainer.train()\n","\n","    print(f\"\\n Evaluating on the test set for {dialect_name}...\")\n","    # --- Improved: Use original dataset for printing ---\n","    original_test_ds = split_dataset_dict[\"test\"]\n","\n","    test_results = trainer.predict(test_ds)\n","    final_bleu_score = test_results.metrics.get('test_bleu', 0.0)\n","    print(f\" Test Set BLEU Score for {dialect_name}: {final_bleu_score:.2f}\")\n","\n","    print(\"\\nüîç Example Translations:\")\n","    predictions = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n","    for i in range(min(5, len(predictions))):\n","        print(f\" Input (Dialect):   {original_test_ds[i]['source']}\")\n","        print(f\" Actual (Standard): {original_test_ds[i]['target']}\")\n","        print(f\" Predicted:         {predictions[i]}\\n\")\n","    # -------------------------------------------------\n","\n","    # Save the final (best) model\n","    trainer.save_model(f\"{output_dir}/best_model\")\n","    print(f\"‚úÖ Best model for {dialect_name} saved to {output_dir}/best_model\")\n","\n","    return trainer\n","\n","\n","# ===============================\n","# üöÄ Training Loop (Improved)\n","# ===============================\n","for dialect in DIALECTS_TO_TRAIN:\n","    split_dataset_dict = create_dataset_dict(dialect)\n","\n","    if split_dataset_dict is None:\n","        continue # Skip if dataset creation failed\n","\n","    tokenized_datasets = tokenize_and_prepare_datasets(split_dataset_dict)\n","\n","    # The train_and_evaluate function now handles model loading and saving\n","    trained_trainer = train_and_evaluate(\n","        dialect,\n","        tokenized_datasets[\"train\"],\n","        tokenized_datasets[\"validation\"],\n","        tokenized_datasets[\"test\"]\n","    )\n","\n","    print(f\"--- Finished processing {dialect} ---\")\n","\n","\n","print(\"\\n‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\")"]},{"cell_type":"markdown","source":["## **NOAKHALI**"],"metadata":{"id":"6dkjx6l03KgZ"}},{"cell_type":"code","source":["# ===============================\n","#  Bangla Dialect ‚Üí Standard Bangla using mBART-50 (Improved)\n","# ===============================\n","\n","!pip install transformers[sentencepiece] datasets sacrebleu evaluate torch pandas openpyxl --quiet\n","\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSeq2SeqLM,\n","    DataCollatorForSeq2Seq,\n","    Seq2SeqTrainingArguments,\n","    Seq2SeqTrainer,\n",")\n","import torch\n","import gc # <-- Added for memory cleanup\n","from google.colab import drive\n","\n","print(\"‚úÖ Libraries installed and imported successfully.\")\n","#drive.mount('/content/drive')\n","\n","# ===============================\n","#  Load Dataset\n","# ===============================\n","\n","try:\n","    df = pd.read_excel(\"/content/bangla_dialect_aligned_18920.xlsx\")\n","    print(\"‚úÖ Successfully loaded the dataset\")\n","except FileNotFoundError:\n","    print(\"‚ö†Ô∏è Dataset not found ‚Äî using sample data.\")\n","    df = pd.DataFrame({\n","        'Standard_Bangla': [\"‡¶∏‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Barisal': [\"‡¶π‡ßá‡¶á ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Chittagong': [\"‡¶π‡ßá‡¶á ‡¶∏‡ßç‡¶ï‡ßã‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"],\n","        'Sylhet': [\"‡¶§‡¶æ‡¶∞‡ßá ‡¶á‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º„ÄÇ\"]\n","    })\n","\n","# Choose dialects to train\n","DIALECTS_TO_TRAIN = ['Noakhali'] # Can now train multiple, one after another\n","\n","# ===============================\n","#  Load Tokenizer (mBART)\n","# ===============================\n","\n","MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n","\n","# Load tokenizer once, it can be reused\n","# For mBART, we set the src/tgt lang on the tokenizer instance\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.src_lang = \"bn_IN\"\n","tokenizer.tgt_lang = \"bn_IN\"\n","\n","print(f\"\\n‚úÖ Tokenizer for '{MODEL_NAME}' loaded.\")\n","print(f\" Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\")\n","\n","\n","# ===============================\n","#  Helper Functions\n","# ===============================\n","\n","def create_dataset_dict(dialect_col):\n","    \"\"\"Creates a preprocessed DatasetDict for a given dialect.\"\"\"\n","    print(f\"\\n--- Processing dialect: {dialect_col} ---\")\n","    df_clean = df[['Standard_Bangla', dialect_col]].dropna()\n","    df_clean = df_clean[\n","        (df_clean['Standard_Bangla'].apply(lambda x: isinstance(x, str) and x.strip() != \"\")) &\n","        (df_clean[dialect_col].apply(lambda x: isinstance(x, str) and x.strip() != \"\"))\n","    ]\n","    subset_df = df_clean.rename(columns={'Standard_Bangla': 'target', dialect_col: 'source'})\n","\n","    if len(subset_df) < 20: # Need enough data to split\n","        print(f\"‚ö†Ô∏è Insufficient data for {dialect_col}. Skipping.\")\n","        return None\n","\n","    hf_dataset = Dataset.from_pandas(subset_df)\n","\n","    # Split, ensuring splits are not too small\n","    train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n","    test_val_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n","\n","    dataset_dict = DatasetDict({\n","        'train': train_test_split['train'],\n","        'validation': test_val_split['train'],\n","        'test': test_val_split['test']\n","    })\n","    print(f\"‚úÖ Dataset splits created for {dialect_col}: Train {len(dataset_dict['train'])}, Val {len(dataset_dict['validation'])}, Test {len(dataset_dict['test'])}\")\n","    return dataset_dict\n","\n","\n","def tokenize_and_prepare_datasets(dataset_dict):\n","    \"\"\"Tokenizes the source and target text in the dataset.\"\"\"\n","\n","    def tokenize_fn(examples):\n","        # mBART tokenizer uses the `src_lang` set on the tokenizer\n","        model_inputs = tokenizer(\n","            examples[\"source\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        # mBART tokenizer uses `tgt_lang` when `text_target` is provided\n","        labels = tokenizer(\n","            text_target=examples[\"target\"],\n","            max_length=128,\n","            truncation=True,\n","            padding=\"max_length\"\n","        )\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    tokenized_datasets = dataset_dict.map(\n","        tokenize_fn,\n","        batched=True,\n","        remove_columns=['source', 'target'] # <-- Added remove_columns\n","    )\n","    print(\"‚úÖ Tokenization complete.\")\n","    return tokenized_datasets\n","\n","\n","def train_and_evaluate(dialect_name, train_ds, val_ds, test_ds):\n","    \"\"\"Initializes and runs the training, then evaluates on the test set.\"\"\"\n","\n","    # --- Memory Cleanup ---\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    # ----------------------\n","\n","    output_dir = f\"/content/Bangla_Dialect_Models/mbart-bangla-{dialect_name.lower()}\"\n","\n","    # --- Load fresh model ---\n","    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n","    # ------------------------\n","\n","    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","    bleu_metric = evaluate.load(\"sacrebleu\")\n","\n","    def compute_metrics(eval_pred):\n","        predictions, labels = eval_pred\n","        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n","        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","        decoded_preds = [pred.strip() for pred in decoded_preds]\n","        decoded_labels = [[label.strip()] for label in decoded_labels]\n","        result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n","        return {\"bleu\": result[\"score\"]}\n","\n","    # --- Improved Training Arguments ---\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        learning_rate=2e-5,\n","\n","        # Use gradient accumulation for a larger effective batch size\n","        per_device_train_batch_size=4,\n","        gradient_accumulation_steps=2, # <-- Effective batch size = 8\n","\n","        per_device_eval_batch_size=8,  # <-- Can be larger for eval\n","        weight_decay=0.01,\n","        save_total_limit=2,\n","\n","        # Increased epochs to avoid underfitting\n","        num_train_epochs=5,           # <-- Set to 5\n","\n","        predict_with_generate=True,\n","        fp16=torch.cuda.is_available(),\n","\n","        # --- CRITICAL FIX: To save the best model ---\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"bleu\",\n","        # --------------------------------------------\n","\n","        # Your \"step\" settings\n","        eval_strategy=\"steps\",\n","        eval_steps=500,\n","        save_steps=500,     # Must match eval_steps\n","        logging_steps=100,\n","\n","        # Standard good practices\n","        warmup_steps=300,\n","        max_grad_norm=1.0,\n","        generation_max_length=128,\n","        generation_num_beams=4,\n","\n","        # Added to hide wandb logs\n","        report_to=\"none\",\n","    )\n","    # -----------------------------------\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=val_ds,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    print(f\"\\n Starting training for {dialect_name} dialect...\")\n","    trainer.train()\n","\n","    print(f\"\\n Evaluating on the test set for {dialect_name}...\")\n","    # --- Improved: Use original dataset for printing ---\n","    original_test_ds = split_dataset_dict[\"test\"]\n","\n","    test_results = trainer.predict(test_ds)\n","    final_bleu_score = test_results.metrics.get('test_bleu', 0.0)\n","    print(f\" Test Set BLEU Score for {dialect_name}: {final_bleu_score:.2f}\")\n","\n","    print(\"\\nüîç Example Translations:\")\n","    predictions = tokenizer.batch_decode(test_results.predictions, skip_special_tokens=True)\n","    for i in range(min(5, len(predictions))):\n","        print(f\" Input (Dialect):   {original_test_ds[i]['source']}\")\n","        print(f\" Actual (Standard): {original_test_ds[i]['target']}\")\n","        print(f\" Predicted:         {predictions[i]}\\n\")\n","    # -------------------------------------------------\n","\n","    # Save the final (best) model\n","    trainer.save_model(f\"{output_dir}/best_model\")\n","    print(f\"‚úÖ Best model for {dialect_name} saved to {output_dir}/best_model\")\n","\n","    return trainer\n","\n","\n","# ===============================\n","# üöÄ Training Loop (Improved)\n","# ===============================\n","for dialect in DIALECTS_TO_TRAIN:\n","    split_dataset_dict = create_dataset_dict(dialect)\n","\n","    if split_dataset_dict is None:\n","        continue # Skip if dataset creation failed\n","\n","    tokenized_datasets = tokenize_and_prepare_datasets(split_dataset_dict)\n","\n","    # The train_and_evaluate function now handles model loading and saving\n","    trained_trainer = train_and_evaluate(\n","        dialect,\n","        tokenized_datasets[\"train\"],\n","        tokenized_datasets[\"validation\"],\n","        tokenized_datasets[\"test\"]\n","    )\n","\n","    print(f\"--- Finished processing {dialect} ---\")\n","\n","\n","print(\"\\n‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5e366a43ccb3434e8dfa424bca031634","015474fc4d0846b9af5d1d86cdcfcc3d","fc563006259c40ef94f8504cc333d211","23149009bdb74ea697b41dd7e43177d9","23de86cbb9324789a2638e396c80f713","8dbfab07e0104bc7b119860b2d543445","7f48eb0cf5ac476eabfb3a713e2fc56e","add367ca350345aba7483d432d454f44","c1db3d7dac2e4f758f572ddb429ad83f","67d482920ae54628894cfc3cae317402","619502725e7c487fb537c1080b4e3b67","a380c16c3a5a4bbcb209932d0c159f4f","f4b74334e58b441ea442339776e2324e","cda3489508f6406281eb7a2ad5bb5413","5ff06313525f452eb04a0972bb5c7a6d","5769dbe3da9c4d89b5c81acf374094d9","ef72c68806c347338d25245c361c4c75","f40f832c8b7b414588a02477c1e4b322","5fd4913ae2734a3880493fd725c42271","21f9c9908b8d47f9a3b0b8943a4e9dbb","2b7f61ac9ac441028b75b72f21314a17","209d026e927c40209f8e134743713d79","24e8fc87ce654754a5c8f9588e6e979b","876850a64e7c411a8659437263f5d37f","cea899de036e48c7b746afad67bcc833","24047987ddc642ddb92571f9fbdfb207","3267d842d74f4699bbd43876baf40908","8304479041084023adf7452862aa58af","553056041a604b7a9a8908ae813334b5","c1d46d96f48c40e9a85b61b31969a2e6","4a86a753a4d84814ae4b9badab9e67b9","94b7430288c14a48950b7f465e1aa7df","64678e9c07b14b36b292ca09e591a8ef","4fbb48bad8f94adeae9bb30b01a63d96","1ac379efa55f4ceb8a28be724941cf8f","bfbfc196f3434ec7bde21db21da13526","156d52899c744a0c8c8d0245ac02eadf","5756df6c34e74779a1f892bc5184a856","6cc517bcebdb407bb466f377f80d66c1","b69c00d8294544c2b8b6dcc074331df6","61a0376616f344e199f2ed8eb1c722bd","d56003d076144e6eb2f0fee5feaab67d","62a914557aff4c19bc4dc7f9f90d0a0f","8929575af9e143f4973f2c057732bb41","e6132bf6e8be4980b23c083e4d193e81","09bfc3a5d47f494a80c9f4363b9a2b6a","b6e928b7e2ed47ba81ae7003ea077e52","97e5fe3434b94a5ba79dcdc864171e57","73f142bb08054f2f931b93407111f663","9094fca3e3e64d74894145fb3ea19480","2809610e99d24e23b8b3e77c363c37f5","edb5c965e8404e6b90c3912580aff005","22222a90b2404568b82c28df028d46aa","9b4b1a0095714fe68af736b562c38fb5","6ab6d111f7714f7e9dc9152ecf97be1c","f36afd0d3bc14d21b9876b4b2a9b8bd8","f15758230b8b4733b8e186f926795404","baa994df573745088021dd046a525e27","aadc7ae882074dc2b0d88095b374b2b6","5353170514a54e069fd7724a5fd3b9fa","54b433ba164f4d099c65137093f2b8b2","65507ed735b74dbf99f305f8eb4fbed9","deeee2f7f9c14ee98e2aca3603ecf855","c8f0d344147749c4a7aa85d3bd6479fb","d9bf7ab3a8254e5298c10f53a7930e9c","22bca499706443c0a38aaf36d65ea72c","9e706d7910554a61bb87612452e526fc","c051b68a3ff8471084fc1a6ab6fb286d","686c584d9a404b8bb2a22922b20a6aa9","6acedde3e2ff4c46a2fe7a3bd502bb8d","3c3f85dfb8df4dd8b44c179b6204bb22","fba99bd5011b451bb8c898f0e2d91b83","9f9be0225fa741a4a324cef6d047b02b","3be359db8651489ead0ceb7e6404c93e","7fb981877b4f4bedb4821f0c7547b0fe","6b3439a9ab494a3a85fdd5a9f5b29475","925f8f8e375c4fa89e64599094ddad0d","bf54fe91a26b4d9592de7406225c4a0d","1b3208206064435f8ab456d133fa1fc1","2cd4fef6175a4f159741c06bc6143071","d308664b8dcb4594a400bdb5012e241c","339fdd238eed40c0a2f6aadb00b2a6ce","fa359182d7cf4a1ea56fa46a9a297e6a","ebc749b4b7814fc2936c5e8f07ff1d8e","8c31e73f34ac45b787075719d7fa3da5","1af990443d7b48a4ab360136838e164f","d62d040a9e0f4c248d1b95e0eb9a37d8","80de4b2aee8e4dcea4a5119a75d87502","b25276bdcdcc43b0be6571a5f21442a9","62997665dbea49898cb1b175c5afd97f","7a76ee694fae46dfb36b44b999e8a253","b9f4c5c2ded94a55b8cff8cf6b6694ca","f0df813403fb40679d08beecc7379260","d0133460cbd54378b9b230286702f3ca","bea22364ccf84300baa69adb069f8bd4","eed62aa2d6464412ad6630a7796ef6b4","6a2a2294311a445f8e41ed2b1224e1f0","c9c38455fe2b421ea60b4f72701ba018","6cfc45f1638845d39d39459edebedf78","78bd32659c9446408bebdc456d456a5a","cd6ef818915d43eeba324d947c8c7f77","17fa641fd3d74fc597d8fc68a94d8663","38f335385fcb489ab1d14f1906c3ece0","1b70a44ae47c47cca42123b4c70622aa","8766670729c341db961fa773987c8772","7594d713e9c442419b7e2b3dc148d27c","db76d7080f184949b7ac6ffe1ca38861","45cef9507cb94c05ada56353910324b1","bc05ef3e9ec945cc8615f2fa3eeb7ee8","5fad85ff13764e9897ec0e54a6794d40"]},"id":"MJakX6DebCFH","outputId":"38fdef66-a568-42bb-b282-0f88a8125743"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h‚úÖ Libraries installed and imported successfully.\n","‚úÖ Successfully loaded the dataset\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e366a43ccb3434e8dfa424bca031634"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a380c16c3a5a4bbcb209932d0c159f4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e8fc87ce654754a5c8f9588e6e979b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fbb48bad8f94adeae9bb30b01a63d96"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Tokenizer for 'facebook/mbart-large-50-many-to-many-mmt' loaded.\n"," Language pair: bn_IN ‚Üí bn_IN (Dialect ‚Üí Standard).\n","\n","--- Processing dialect: Noakhali ---\n","‚úÖ Dataset splits created for Noakhali: Train 2000, Val 250, Test 250\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6132bf6e8be4980b23c083e4d193e81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/250 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f36afd0d3bc14d21b9876b4b2a9b8bd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/250 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e706d7910554a61bb87612452e526fc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ Tokenization complete.\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf54fe91a26b4d9592de7406225c4a0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25276bdcdcc43b0be6571a5f21442a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78bd32659c9446408bebdc456d456a5a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4142675496.py:191: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Starting training for Noakhali dialect...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1250/1250 35:49, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Bleu</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.162300</td>\n","      <td>0.151910</td>\n","      <td>28.633643</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.059100</td>\n","      <td>0.140994</td>\n","      <td>35.489501</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n","There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Evaluating on the test set for Noakhali...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stdout","text":[" Test Set BLEU Score for Noakhali: 39.56\n","\n","üîç Example Translations:\n"," Input (Dialect):   ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ‡¶∞ ‡¶≤‡¶ó‡ßá ‡¶≤‡¶ó‡ßá ‡¶ï‡ßÄ ‡¶è‡¶ï‡¶ï‡¶æ‡¶® ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶ï‡¶á‡¶∞‡¶≤‡ßã\n"," Actual (Standard): ‡¶Ø‡¶æ‡¶ì‡ßü‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡ßÄ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶ï‡¶∞‡¶≤\n"," Predicted:         ‡¶Ø‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡ßÄ ‡¶è‡¶ï‡¶ü‡¶æ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶ï‡¶∞‡¶≤‡ßã\n","\n"," Input (Dialect):   ‡¶§‡ßÅ‡¶á ‡¶ï‡¶ø ‡¶Ü‡¶∞‡ßá ‡¶è‡¶á ‡¶ï‡¶æ‡¶Æ ‡¶Ü‡¶® ‡¶ï‡¶∞‡¶ø ‡¶¶‡¶ø‡¶§‡¶æ ‡¶π‡¶æ‡¶á‡¶∞‡¶¨‡¶æ ‡¶®‡¶ø?\n"," Actual (Standard): ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶è‡¶á ‡¶ï‡¶æ‡¶ú‡¶ü‡¶ø ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá?\n"," Predicted:         ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶è‡¶á ‡¶ï‡¶æ‡¶ú‡¶ü‡¶ø ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá ?\n","\n"," Input (Dialect):   ‡¶§‡ßã‡¶∞ ‡¶π‡¶∞‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶¨‡ßá?\n"," Actual (Standard): ‡¶§‡ßã‡¶∞ ‡¶™‡¶∞‡¶ø‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶¨‡ßá ?\n"," Predicted:         ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶™‡¶õ‡¶®‡ßç‡¶¶ ‡¶ï‡¶¨‡ßá?\n","\n"," Input (Dialect):   ‡¶π‡ßá‡¶§‡ßá ‡¶Æ‡ßá‡¶≤‡¶æ ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶ó‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶Ö‡¶ó‡¶ó‡¶æ ‡¶¨‡¶á‡¶§‡¶æ‡¶≤‡¶ø‡¶∞‡ßá ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶á‡¶∏‡¶§‡ßã\n"," Actual (Standard): ‡¶∏‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶ó‡ßá ‡¶§‡¶æ‡¶∞ ‡¶¨‡¶ø‡¶¶‡ßç‡¶Ø‡¶æ‡¶≤‡ßü‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Æ‡ßá‡ßü‡ßá‡¶ï‡ßá ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶§‡ßã\n"," Predicted:         ‡¶∏‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¶‡¶ø‡¶® ‡¶Ü‡¶ó‡ßá ‡¶∏‡ßç‡¶ï‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡ßã‡¶§‡¶æ‡¶≤‡¶ø‡¶ï‡ßá ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶§‡ßã\n","\n"," Input (Dialect):   ‡¶ï‡ßã‡¶®‡ßã ‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶ì‡¶Ø‡¶º‡¶æ‡¶∂‡¶æ ‡¶π‡¶æ‡¶®‡¶ø ‡¶¶‡ßá‡¶Ø‡¶º‡¶∞ ‡¶®‡¶æ\n"," Actual (Standard): ‡¶ï‡ßã‡¶®‡ßã‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶ì‡ßü‡¶æ‡¶∏‡¶æ ‡¶™‡¶æ‡¶®‡¶ø ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ\n"," Predicted:         ‡¶ï‡ßã‡¶®‡ßã ‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶Ü‡¶Æ‡¶∂‡¶æ ‡¶™‡¶æ‡¶®‡¶ø ‡¶¶‡ßá‡¶Ø‡¶º ‡¶®‡¶æ\n","\n","‚úÖ Best model for Noakhali saved to /content/Bangla_Dialect_Models/mbart-bangla-noakhali/best_model\n","--- Finished processing Noakhali ---\n","\n","‚úÖ Finished training all dialect models. Best models are saved in Google Drive.\n"]}]}]}